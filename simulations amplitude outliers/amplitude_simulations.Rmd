---
title: "OUTLIER DETECTION AMPLITUDE"
author: "Josué Cerpa Araña"
date: "21 de agosto de 2019"
output: pdf_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE)
```

```{r results='hide', message=FALSE, warning=FALSE}
library(rockchalk) #For mvrnorm
library(parallel)
library(doParallel)
library(foreach)
library(Matrix)
library(ggplot2)
require(reshape2)
library(robustbase)
library(gridExtra)
library(WRS2) #for ben and winsor
library(ccaPP) #for quandrant correlation
library(DescTools)
library(asbio)
```


```{r}
find.tangent.X.intercept <- function(x, y, which.x=which.max(diff(y))+1, plot=F){
  spl <- smooth.spline(y ~ x)
  #which.x <- which.max(diff(y))+1 # which.max(x)
  newx.0 <- x[which.min(diff(diff(y)))]
  newx.1 <- mean(x[c(which.x-1, which.x)], na.rm=TRUE)
  pred0 <- predict(spl, x=newx.0, deriv=0)
  pred1 <- predict(spl, x=newx.1, deriv=1)
  
  #slope correction
  pred1$y <- max(pred1$y, 1.5)
  
  y.intercept <- pred0$y - (pred1$y*newx.0)
  x.intercept <- -y.intercept/pred1$y
  
  if( plot ) {
    plot(x, y, type="l", ylim=c(0,max(y)))
    abline(h=0, col=8)
    lines(spl, col=2) # spline
    points(pred0, col=2, pch=19) # point to predict tangent 
    lines(x, y.intercept + pred1$y*x, col=3) # tangent (1st deriv. of spline at newx)
    points(x.intercept, 0, col=3, pch=19) # x intercept
  }
  x.intercept
}

compute_tangent_cutoff <- function(metric, plot=F){
  cpoint <- find.tangent.X.intercept(seq(0,1,length=length(metric))
                                     , metric
                                     #, newx = newx
                                     , plot = plot)
  ceiling(cpoint * length(metric))
}

getOutlierCutoff <- function(curve, method=c("tangent", "deriv", "deriv-enh", "deriv.old", "roc")
                             , slope=slope, curveName="", plot=FALSE){
  method <- match.arg(method)
  
  curve.seq <- seq(1 / length(curve), 1, by = 1 / length(curve))
  
  if( method == "tangent" ){
    which_best <- compute_tangent_cutoff(curve, plot = plot)
  } else if( method == "roc" ){ #compute the furthest point between the extreme points
    which_best <- getFurtherPoint(curve)
  }else if( method == "deriv" ){ # compute elbow point using 1st derivative
    #curve.norm2 <- curve / sqrt(sum(curve^2)) # norm-2
    curve.norm2 <- curve / max(curve) # norm infty
    targetPoint <- compute_exp_elbow(curve.norm2, slope=slope, enhanced=F, plot=plot) # normalized curve as param
    which_best <- min(targetPoint, length(curve)+1)
  }else if( method == "deriv-enh" ){ # compute elbow point using 1st derivative
    #curve.norm2 <- curve / sqrt(sum(curve^2)) # norm-2
    curve.norm2 <- curve / max(curve) # norm infty
    targetPoint <- compute_exp_elbow(curve.norm2, slope=slope, enhanced=T, plot=plot) # normalized curve as param
    which_best <- min(targetPoint, length(curve)+1)
  }else if( method == "deriv.old" ){
    curve.norm2 <- curve / max(curve) # norm infty
    elbow.point <- compute_exp_elbow.last(curve.norm2, slope=slope, plot=plot) # normalized curve as param
    metric.seq <- seq(1 / length(curve), 1, by = 1 / length(curve))
    which_best <- min(which(metric.seq > elbow.point), length(curve)+1)
  }
  elbow.point <- curve.seq[which_best]
  cutoff <- curve[which_best]
  if( plot ) {
    plot(curve.seq, curve, type="l", log=""
         , main=paste0("elbow cut @", (1 - elbow.point) * 100
                       , "% [", curveName, ifelse(exists("threshold"), paste0("; >=",threshold), ""), "]"))
    points(elbow.point, cutoff, col="red", pch=4)
  }
  cutoff
}
```

```{r}
mod.02.amp = function(n, m, alpha){
  domain = seq(0, 2*pi,length=m)
  mu.add1 = sin(domain)
  mu.add2 = cos(domain)
  n.out = rbinom(1, n, alpha)
  n.good = n - n.out
  if(n.out > 0){
    outs = as.integer(seq(from = (n - n.out + 1), to = n, by = 1))
    a1 = runif(n.good, 0.75, 1.25)
    a2 = runif(n.good, 0.75, 1.25)
    sample.good = (a1%*%t(mu.add1)) + (a2%*%t(mu.add2))
    a1 = runif(n.out, 1.30, 1.50)
    a2 = runif(n.out, 1.30, 1.50)
    sample.out = (a1%*%t(mu.add1)) + (a2%*%t(mu.add2))
    sample.all = rbind(sample.good, sample.out)
    rownames(sample.all)[outs]=c("amp")
    rownames(sample.all)[-outs]=c("user")
  } else {
    outs = integer(0)
    a1 = runif(n.good, 0.75, 1.25)
    a2 = runif(n.good, 0.75, 1.25)
    sample.all = (a1%*%t(mu.add1)) + (a2%*%t(mu.add2))
  }
  return(list("n.out"=n.out, "outs"=outs, "sample"=sample.all))
}
```

#Amplitude indexes

##Index MUOD amp

$$I_{A_{1}}=\left|\frac{1}{n} \sum_{j=1}^{n} \frac{Cov(x,x_j)}{var(x_j)}-1 \right|$$ 

```{r}
iamp.default= function(mtx,cores=detectCores(),partition=ncol(mtx)/cores){
  #Number of splits for the parallel processing
  splits <- parallel:::splitList(1:ncol(mtx), max(1, as.integer(partition)))
  #Number of clusters in terms of the cores
  cluster = makeCluster(cores)
  #Necessary to register the parallel backendn
  registerDoParallel(cluster)
  #Variance of each user (by columns)
  variance=parApply(cluster,mtx,2,var)
  #is: computes a matrix which each column is the (cov(x,x_j)/var(x_j))-1
  is=foreach(i=1:length(splits),.combine = 'cbind')  %dopar% {
    #covx.varxj calculates cov(x,x_j)/var(x_j)
    covx.varxj=data.frame(cov(mtx,mtx[,splits[[i]]],use="pairwise.complete.obs"))/
      matrix(variance,ncol=1)
    #computes (cov(x,x_j)/var(x_j)) - 1
    result=covx.varxj-1
  }
  #ia:applies the the column mean and the absolute value 
  #for the square matrix is
  ia=foreach(i=1:length(splits),.combine = 'c')  %dopar% {
   abs(colMeans(is[,splits[[i]]]))
 }
 stopCluster(cluster)
 names(ia)[grepl("amp",names(ia))]="amp"
 names(ia)[grepl("user",names(ia))]="user"
 return(ia)
}
```

##Index tau-kendall per MAD

$$I_{A_{2}}=\left|\frac{1}{n} \sum_{j=1}^{n} \tau(x,x_j) \cdot MAD(x) \right|$$

```{r}
iamp.kendall =  function(mtx,cores=detectCores(),partition=ncol(mtx)/cores){
  #Number of splits for the parallel processing
  splits <- parallel:::splitList(1:ncol(mtx), max(1, as.integer(partition)))
  #Number of clusters in terms of the cores
  cluster = makeCluster(cores)
  #Necessary to register the parallel backendn
  registerDoParallel(cluster)
  #is:computes the column mean for the kendall-tau correlation
  is=foreach(i=1:length(splits),.combine = 'c')  %dopar% {
    colMeans(
      cor(mtx, mtx[,splits[[i]]],method = "kendall",use="pairwise.complete.obs"), 
      na.rm=T)
  }
  #MAD of each user (by columns)
  MAD=parApply(cl=cluster,MARGIN=2,X=mtx,FUN=mad)
  #ia:computes tau(x,xj)*MAD(x)
  ia=foreach(i=1:length(splits),.combine = 'c')  %dopar% {
    abs(is[splits[[i]]] * (MAD[splits[[i]]]/1.4826))
  }
  stopCluster(cluster)
  #result=data.frame(kendall=names(sort(ia)),Iampkendall=sort(ia))
  #result$kendall[grepl("a",result$kendall)]="amp"
  names(ia)[grepl("amp",names(ia))]="amp"
  names(ia)[grepl("user",names(ia))]="user"
  return(ia)
}
```

## Index biweight per MAD

```{r}
iamp.biweight =  function(mtx,cores=detectCores(),partition=ncol(mtx)/cores){
  #Number of clusters in terms of the cores
  cluster = makeCluster(cores)
  #Necessary to register the parallel backendn
  registerDoParallel(cluster)
  #Number of splits for the parallel processing
  splits <- parallel:::splitList(1:ncol(mtx), max(1, as.integer(partition)))
  #Users' median
  MEDIAN=foreach(i=1:length(splits),.combine='c') %dopar% {
    apply(mtx[,splits[[i]]],2,median)
  }
  #Users' mad
  MAD=foreach(i=1:length(splits),.combine='c') %dopar% {
    apply(mtx[,splits[[i]]],2,mad)
  }
  #Computation biweight midcovariance#######################
  ui=foreach(i=1:length(splits),.combine='cbind') %dopar% {
    numerator=t(mtx[,splits[[i]]])-MEDIAN[splits[[i]]]
    denominator=9*qnorm(0.75)*MAD[splits[[i]]]
    result=t(numerator/denominator)
  }
  ai=foreach(i=1:length(splits),.combine='cbind') %dopar% {
    I1=ui[,splits[[i]]]
    I1[(-1<=I1) & (I1<=1)]=1
    I1[(I1< -1) | I1 > 1]=0
    I1
  }
  minusmedian=foreach(i=1:length(splits),.combine='cbind') %dopar% {
    #In ordert to compute user-median(user) we need to make the transpose
    #of the user's matrix and the end we transpose again.
    t(t(mtx[,splits[[i]]])-MEDIAN[splits[[i]]])
  }
  biwmidcov=foreach(i=1:ncol(mtx),.combine='rbind') %:%
    foreach(j=1:length(splits),.combine='c') %dopar% {
    #Elements numerator############  
    ai.x=matrix(ai[,i],ncol = 1)
    minusmedian.x=matrix(minusmedian[,i],ncol=1)
    ui.x=(1-matrix(ui[,i],ncol=1)^2)
    bi.y=data.frame(ai[,splits[[j]]])
    minusmedian.y=data.frame(minusmedian[,splits[[j]]])
    vi.y=data.frame((1-ui[,splits[[j]]]^2))
    
    #Elements denominator###########
    ui5.x=(1-5*matrix(ui[,i],ncol=1)^2)
    vi5.y=data.frame((1-5*ui[,splits[[j]]]^2))
    
    #biweight midcovariance
    numerator.1=ai.x * minusmedian.x * ui.x^2 * bi.y * minusmedian.y *vi.y^2
    numerator.1.1=nrow(mtx)*apply(numerator.1,2,sum)
    
    denominator.1=sum(ai.x * ui.x * ui5.x)
    denominator.2=bi.y * vi.y * vi5.y 
    denominator.2.1=apply(denominator.2,2,sum)
    
    sbxy=numerator.1.1/(denominator.1 * denominator.2.1)
    }
  ai.midvariance=foreach(i=1:length(splits),.combine='cbind') %dopar% {
    I1=abs(ui[,splits[[i]]])
    I1[I1<1]=1
    I1[!(I1=1)]=0
    I1
  }
  biwmidvar=foreach(i=1:length(splits),.combine='c') %dopar% {
    numerator1=ai.midvariance[,splits[[i]]]*minusmedian[,splits[[i]]]^2 *(1-ui[,splits[[i]]]^2)^4
    numerator1.1=sqrt(apply(numerator1,2,sum))
    
    denominator1=ai.midvariance[,splits[[i]]] * (1-ui[,splits[[i]]]^2) * (1-5*ui[,splits[[i]]]^2)
    denominator1.1=abs(apply(denominator1,2,sum))
    
    ((sqrt(nrow(mtx))*numerator1.1)/denominator1.1)^2
  }
  
  biwmidcor=foreach(i=1:ncol(mtx),.combine='rbind') %:%
    foreach(j=1:ncol(mtx),.combine='c') %dopar% {
      biwmidcov[i,j]/sqrt(biwmidvar[i]*biwmidvar[j])
    }
  is=foreach(i=1:length(splits),.combine='c') %dopar% {
    colMeans(biwmidcor[,splits[[i]]])
  }
  ia=foreach(i=1:length(splits),.combine = 'c')  %dopar% {
    abs(is[splits[[i]]] * (MAD[splits[[i]]]))
    }
  stopCluster(cluster)
  names(ia)=colnames(mtx)
  return(ia)
}
```

```{r}
results=function(x,datos,totalout,tipo){
  #Names' assigning of outliers and users
  names(x)=colnames(datos)
  
  #Sort of the index in increasing order
  x=sort(x)
  
  #Different cutting methods############################################
    #Boxplot method
  out.boxplot=x[x>boxplot.stats(x)$stats[5]]
  
    #Adjusted boxplot
  out.adjboxplot=x[x>adjboxStats(x)$fence[2]]
  
    #Adjusted outlyingness
  AO=NULL
  for(j in 1:length(x)){
    if(x[j]>median(x)){
    AO[j]=(x[j]-median(x))/(adjboxStats(x)$stats[5]-median(x))
    }else{
      AO[j]=(median(x)-x[j])/(median(x)-adjboxStats(x)$stats[1])
    }
    }
  out.adjout=AO[AO>adjboxStats(AO)$fence[2]]
  
    #Rules Based on Means and Variances
  meanvar=(x-mean(x))/sd(x)
  out.meanvar=meanvar[meanvar>2.24]
  
    #Carling's Modification
  n=length(x)
  k=(17.63*n-23.64)/(7.74*n -3.71)
  j=floor(n/4 + 5/12) 
  h=n/4 + 5/12 - j
  q1=(1-h)*x[j] + h*x[j+1]
  k.q2=n-j+1
  q2=(1-h)*x[k.q2] + h * x[k.q2-1]
  cutoff.carling=median(x)+k*(q2-q1)
  out.carlings=x[x>cutoff.carling]
  
    #MAD-median rule
  madmedian=(0.6745*(abs(x-median(x))))/mad(x)
  out.madmedian=madmedian[madmedian>2.24]
  
    #Method Based on the Interquartile Range (ideal fourths)
  cutoff.ifourths=q2 + 1.5*(q2 - q1)
  out.ifourths=x[x>cutoff.ifourths]
  
    #Tangent method
  cutoff.tangent=getOutlierCutoff(x,"tangent")
  out.tangent=x[x>cutoff.tangent]
  
  #Measures c, f and F for each type of cut#################################
  
  #BOXPLOT##################################################################
  #Correct outlier detection percentage
  c.boxplot=c((length(which(names(out.boxplot)==tipo))/totalout)*100)
  #False outlier detection percentage
  f.boxplot=c((length(which(names(out.boxplot)!=tipo))/(n-totalout))*100)
  #False positives
  FP.boxplot=c(length(which(names(out.boxplot)!=tipo)))
  #False negatives
  FN.boxplot=c(totalout-length(which(names(out.boxplot)==tipo)))
  #True positives
  TP.boxplot=c(length(which(names(out.boxplot)==tipo)))
  #F measure
  R.boxplot=c(TP.boxplot/(TP.boxplot+FN.boxplot))
  P.boxplot=c(TP.boxplot/(TP.boxplot+FP.boxplot))
  F.measure.boxplot=c((2*R.boxplot*P.boxplot)/(R.boxplot+P.boxplot))
  
  #ADJUSTED BOXPLOT#########################################################
  
  #Correct outlier detection percentage
  c.adjboxplot=c((length(which(names(out.adjboxplot)==tipo))/totalout)*100)
  #False outlier detection percentage
  f.adjboxplot=c((length(which(names(out.adjboxplot)!=tipo))/(n-totalout))*100)
  #False positives
  FP.adjboxplot=c(length(which(names(out.adjboxplot)!=tipo)))
  #False negatives
  FN.adjboxplot=c(totalout-length(which(names(out.adjboxplot)==tipo)))
  #True positives
  TP.adjboxplot=c(length(which(names(out.adjboxplot)==tipo)))
  #F measure
  R.adjboxplot=c(TP.adjboxplot/(TP.adjboxplot+FN.adjboxplot))
  P.adjboxplot=c(TP.adjboxplot/(TP.adjboxplot+FP.adjboxplot))
  F.measure.adjboxplot=c((2*R.adjboxplot*P.adjboxplot)/(R.adjboxplot+P.adjboxplot))
  
  #ADJUSTED OUTLYINGNESS#########################################################
  #Correct outlier detection percentage
  c.adjout=c((length(which(names(out.adjout)==tipo))/totalout)*100)
  #False outlier detection percentage
  f.adjout=c((length(which(names(out.adjout)!=tipo))/(n-totalout))*100)
  #False positives
  FP.adjout=c(length(which(names(out.adjout)!=tipo)))
  #False negatives
  FN.adjout=c(totalout-length(which(names(out.adjout)==tipo)))
  #True positives
  TP.adjout=c(length(which(names(out.adjout)==tipo)))
  #F measure
  R.adjout=c(TP.adjout/(TP.adjout+FN.adjout))
  P.adjout=c(TP.adjout/(TP.adjout+FP.adjout))
  F.measure.adjout=c((2*R.adjout*P.adjout)/(R.adjout+P.adjout))
  
  #RULE BASED ON MEAN AND VARIANCES##############################################
  #Correct outlier detection percentage
  c.meanvar=c((length(which(names(out.meanvar)==tipo))/totalout)*100)
  #False outlier detection percentage
  f.meanvar=c((length(which(names(out.meanvar)!=tipo))/(n-totalout))*100)
  #False positives
  FP.meanvar=c(length(which(names(out.meanvar)!=tipo)))
  #False negatives
  FN.meanvar=c(totalout-length(which(names(out.meanvar)==tipo)))
  #True positives
  TP.meanvar=c(length(which(names(out.meanvar)==tipo)))
  #F measure
  R.meanvar=c(TP.meanvar/(TP.meanvar+FN.meanvar))
  P.meanvar=c(TP.meanvar/(TP.meanvar+FP.meanvar))
  F.measure.meanvar=c((2*R.meanvar*P.meanvar)/(R.meanvar+P.meanvar))
  #CARLINGS##################################################################
  #Correct outlier detection percentage
  c.carlings=c((length(which(names(out.carlings)==tipo))/totalout)*100)
  #False outlier detection percentage
  f.carlings=c((length(which(names(out.carlings)!=tipo))/(n-totalout))*100)
  #False positives
  FP.carlings=c(length(which(names(out.carlings)!=tipo)))
  #False negatives
  FN.carlings=c(totalout-length(which(names(out.carlings)==tipo)))
  #True positives
  TP.carlings=c(length(which(names(out.carlings)==tipo)))
  #F measure
  R.carlings=c(TP.carlings/(TP.carlings+FN.carlings))
  P.carlings=c(TP.carlings/(TP.carlings+FP.carlings))
  F.measure.carlings=c((2*R.carlings*P.carlings)/(R.carlings+P.carlings))
  
  ##MADMEDIAN##################################################################
  #Correct outlier detection percentage
  c.madmedian=c((length(which(names(out.madmedian)==tipo))/totalout)*100)
  #False outlier detection percentage
  f.madmedian=c((length(which(names(out.madmedian)!=tipo))/(n-totalout))*100)
  #False positives
  FP.madmedian=c(length(which(names(out.madmedian)!=tipo)))
  #False negatives
  FN.madmedian=c(totalout-length(which(names(out.madmedian)==tipo)))
  #True positives
  TP.madmedian=c(length(which(names(out.madmedian)==tipo)))
  #F measure
  R.madmedian=c(TP.madmedian/(TP.madmedian+FN.madmedian))
  P.madmedian=c(TP.madmedian/(TP.madmedian+FP.madmedian))
  F.measure.madmedian=c((2*R.madmedian*P.madmedian)/(R.madmedian+P.madmedian))
  
  ##IDEAL FOURTHS################################################################
  #Correct outlier detection percentage
  c.ifourths=c((length(which(names(out.ifourths)==tipo))/totalout)*100)
  #False outlier detection percentage
  f.ifourths=c((length(which(names(out.ifourths)!=tipo))/(n-totalout))*100)
  #False positives
  FP.ifourths=c(length(which(names(out.ifourths)!=tipo)))
  #False negatives
  FN.ifourths=c(totalout-length(which(names(out.ifourths)==tipo)))
  #True positives
  TP.ifourths=c(length(which(names(out.ifourths)==tipo)))
  #F measure
  R.ifourths=c(TP.ifourths/(TP.ifourths+FN.ifourths))
  P.ifourths=c(TP.ifourths/(TP.ifourths+FP.ifourths))
  F.measure.ifourths=c((2*R.ifourths*P.ifourths)/(R.ifourths+P.ifourths))
  
  ##TANGENT#######################################################################
  #Correct outlier detection percentage
  c.tangent=c((length(which(names(out.tangent)==tipo))/totalout)*100)
  #False outlier detection percentage
  f.tangent=c((length(which(names(out.tangent)!=tipo))/(n-totalout))*100)
  #False positives
  FP.tangent=c(length(which(names(out.tangent)!=tipo)))
  #False negatives
  FN.tangent=c(totalout-length(which(names(out.tangent)==tipo)))
  #True positives
  TP.tangent=c(length(which(names(out.tangent)==tipo)))
  #F measure
  R.tangent=c(TP.tangent/(TP.tangent+FN.tangent))
  P.tangent=c(TP.tangent/(TP.tangent+FP.tangent))
  F.measure.tangent=c((2*R.tangent*P.tangent)/(R.tangent+P.tangent))
  ###############
  return(list(cbind(c.boxplot,f.boxplot,F.measure.boxplot,
                    c.adjboxplot,f.adjboxplot,F.measure.adjboxplot,
                    c.adjout,f.adjout,F.measure.adjout,
                    c.meanvar,f.meanvar,F.measure.meanvar,
                    c.carlings,f.carlings,F.measure.carlings,
                    c.madmedian,f.madmedian,F.measure.madmedian,
                    c.ifourths,f.ifourths,F.measure.ifourths,
                    c.tangent,f.tangent,F.measure.tangent)))
}

summarizing=function(MATRIX,cores=detectCores()){
  cluster=makeCluster(cores)
  registerDoParallel(cluster)
  newdata=parApply(cl=cluster,X=MATRIX,MARGIN=2,
                    FUN = function(x) ifelse(is.nan(x)==TRUE,0,x))
  newdata2=parApply(cl=cluster,X=newdata,MARGIN=2,FUN = mean)
  stopCluster(cluster)
  return(newdata2)
}
```

```{r}
muod=function(mod,sim=100,alpha=0.05,cols=100,rows=25,cores=detectCores()){
  i=1
  if(mod==1){
      m.pearson=m.kendall=m.spearman=m.bend=m.winsor=m.quadrant=m.robustreg=
    m.mediancor=m.madcor=m.biweight=matrix(,ncol = 24,nrow = sim)
  }
  if(mod==2){
      m.muodamp=m.kendall=m.biweight=matrix(,ncol = 24,nrow = sim)
  }
  
  while(i<(sim+1)){
    if(mod==1){
      #set.seed(1)
      #Parameters of the data
      model=mod.03.sha(cols,rows,alpha)
      data=t(model$sample)
      nout=model$n.out
      out="shape"
      if(nout!=0){
        #Indexes
      pearson=is.pearson(data)
      kendall=is.kendall(data)
      spearman=is.spearman(data)
      bend=is.bend(data)
      winsor=is.winsor(data)
      quadrant=is.quadrant(data)
      robustreg=is.robustcor(data)
      mediancor=is.median(data)
      madcor=is.mad(data)
      biweight=is.biweight(data)
      indexes=data.frame(pearson=pearson,kendall=kendall,spearman=spearman,
                         bend=bend,winsor=winsor,quadrant=quadrant,
                         robustreg=robustreg,mediancor=mediancor,madcor=madcor,
                         biweight=biweight)
      
      #Computation of the different measurements
      #Number of clusters in terms of the cores
      #cluster = makeCluster(cores)
      #Necessary to register the parallel backendn
      #registerDoParallel(cluster)
      #Computation of the different measurements
      R=apply(X=indexes,MARGIN=2,FUN = results,data,nout,out)
      #Closing the cluster
      #stopCluster(cluster)
      m.pearson[i,]=R$pearson[[1]]
      m.kendall[i,]=R$kendall[[1]]
      m.spearman[i,]=R$spearman[[1]]
      m.bend[i,]=R$bend[[1]]
      m.winsor[i,]=R$winsor[[1]]
      m.quadrant[i,]=R$quadrant[[1]]
      m.robustreg[i,]=R$robustreg[[1]]
      m.mediancor[i,]=R$mediancor[[1]]
      m.madcor[i,]=R$madcor[[1]]
      m.biweight[i,]=R$biweight[[1]]
      i=i+1
      }
    }
    
    if(mod==2){
            #set.seed(1)
      #Parameters of the data
      model=mod.02.amp(cols,rows,alpha)
      data=t(model$sample)
      nout=model$n.out
      out="amp"
      if(nout!=0){
        #Indexes
      muodamp=iamp.default(data)
      muodamp=(muodamp-min(muodamp))/(max(muodamp)-min(muodamp))
      kendall=iamp.kendall(data)
      kendall=(kendall-min(kendall))/(max(kendall)-min(kendall))
      biweight=iamp.biweight(data)
      biweight=(biweight-min(biweight))/(max(biweight)-min(biweight))
      indexes=data.frame(muodamp=muodamp,kendall=kendall,
                         biweight=biweight)
      
      #Computation of the different measurements
      #Number of clusters in terms of the cores
      #cluster = makeCluster(cores)
      #Necessary to register the parallel backendn
      #registerDoParallel(cluster)
      #Computation of the different measurements
      R=apply(X=indexes,MARGIN=2,FUN = results,data,nout,out)
      #Closing the cluster
      #stopCluster(cluster)
      m.muodamp[i,]=R$muodamp[[1]]
      m.kendall[i,]=R$kendall[[1]]
      m.biweight[i,]=R$biweight[[1]]
      i=i+1
      }
  }
  }
  
  if(mod==1){
  namescol=c("c","f","F")
  namesrow=c("boxplot","adjboxplot","adjout","meanvar","carlings","madmedian","ifourths","tangent")
  pearson.clean=matrix(summarizing(m.pearson),nrow=8,ncol=3,byrow = TRUE,dimnames= list(namesrow,namescol))
  kendall.clean=matrix(summarizing(m.kendall),nrow=8,ncol=3,byrow = TRUE,dimnames= list(namesrow,namescol))
  spearman.clean=matrix(summarizing(m.spearman),nrow=8,ncol=3,byrow = TRUE,dimnames= list(namesrow,namescol))
  bend.clean=matrix(summarizing(m.bend),nrow=8,ncol=3,byrow = TRUE,dimnames= list(namesrow,namescol))
  winsor.clean=matrix(summarizing(m.winsor),nrow=8,ncol=3,byrow = TRUE,dimnames= list(namesrow,namescol))
  quadrant.clean=matrix(summarizing(m.quadrant),nrow=8,ncol=3,byrow = TRUE,dimnames= list(namesrow,namescol))
  robustreg.clean=matrix(summarizing(m.robustreg),nrow=8,ncol=3,byrow = TRUE,dimnames= list(namesrow,namescol))
  mediancor.clean=matrix(summarizing(m.mediancor),nrow=8,ncol=3,byrow = TRUE,dimnames= list(namesrow,namescol))
  madcor.clean=matrix(summarizing(m.madcor),nrow=8,ncol=3,byrow = TRUE,dimnames= list(namesrow,namescol))
  biweight.clean=matrix(summarizing(m.biweight),nrow=8,ncol=3,byrow = TRUE,dimnames= list(namesrow,namescol))
  
    return(list(MEASURES.PEARSON=pearson.clean,
              MEASURES.KENDALL=kendall.clean,
              MEASURES.SPEARMAN=spearman.clean,
              MEASURES.BEND=bend.clean,
              MEASURES.WINSOR=winsor.clean,
              MEASURES.QUADRANT=quadrant.clean,
              MEASURES.ROBUSTREG=robustreg.clean,
              MEASURES.MEDIANCOR=mediancor.clean,
              MEASURES.MADCOR=madcor.clean,
              MEASURES.BIWEIGHT=biweight.clean))
  }
  if(mod==2){
      namescol=c("c","f","F")
  namesrow=c("boxplot","adjboxplot","adjout","meanvar","carlings","madmedian","ifourths","tangent")
  muodamp.clean=matrix(summarizing(m.muodamp),nrow=8,ncol=3,byrow = TRUE,dimnames= list(namesrow,namescol))
  kendall.clean=matrix(summarizing(m.kendall),nrow=8,ncol=3,byrow = TRUE,dimnames= list(namesrow,namescol))
  biweight.clean=matrix(summarizing(m.biweight),nrow=8,ncol=3,byrow = TRUE,dimnames= list(namesrow,namescol))
  return(list(MEASURES.MUODAMP=muodamp.clean,
              MEASURES.KENDALL=kendall.clean,
              MEASURES.BIWEIGHT=biweight.clean))
    
  }
  
}
```

```{r}
muod(mod=2)
```


